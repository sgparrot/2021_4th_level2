{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210903_Level2_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z56VcUB8J4ra"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install textblob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvDrFIZK8Bos"
      },
      "source": [
        "import re, os, nltk, konlpy, random, spacy, textblob\n",
        "from konlpy.tag import Kkma, Okt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E_JcsbKL49h"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('wordnet')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB0rumA1K-1p",
        "outputId": "61bbe44c-1525-4281-ccc2-bc006b2d95ff"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(\"Hi. Thank you for choosing Mundo. What seem to be problem today?\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi.', 'Thank you for choosing Mundo.', 'What seem to be problem today?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtC_Y9HOLgLD",
        "outputId": "3842dcaf-ae55-47a2-8bf7-7dacf9f95a39"
      },
      "source": [
        "#특징 찾기1\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "text=\"Me sorry, Cap'ain. You got the scurvy. Stage 12!\"\n",
        "x=word_tokenize(text)\n",
        "print(x)\n",
        "pos_tag(x)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Me', 'sorry', ',', \"Cap'ain\", '.', 'You', 'got', 'the', 'scurvy', '.', 'Stage', '12', '!']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Me', 'NNP'),\n",
              " ('sorry', 'NN'),\n",
              " (',', ','),\n",
              " (\"Cap'ain\", 'NNP'),\n",
              " ('.', '.'),\n",
              " ('You', 'PRP'),\n",
              " ('got', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('scurvy', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Stage', 'NN'),\n",
              " ('12', 'CD'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9XEsYF2dFRh"
      },
      "source": [
        "!spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gir10j_TUVek",
        "outputId": "03da0df6-85c9-4381-bc05-aeb7b4c57fc5"
      },
      "source": [
        "#특징 찾기2 Lemmatizing\n",
        "#표제어: 기본 사전형 단어(뿌리 단어를 찾아 개수 줄이기 위해)\n",
        "#https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "\n",
        "text=\"Me sorry, Cap'ain. You got the scurvy. Stage 12!\"\n",
        "wnetx=word_tokenize(text)\n",
        "wordnetlem=WordNetLemmatizer()\n",
        "\n",
        "spacylem = spacy.load('en', disable=['parser', 'ner'])\n",
        "print(\"wnet: \", [wordnetlem.lemmatize(z) for z in wnetx])\n",
        "print(\"spcy: \", [z.lemma_ for z in spacylem(text)])\n",
        "print(\"blob: \", [z.lemmatize() for z in TextBlob(text).words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wnet:  ['Me', 'sorry', ',', \"Cap'ain\", '.', 'You', 'got', 'the', 'scurvy', '.', 'Stage', '12', '!']\n",
            "spcy:  ['-PRON-', 'sorry', ',', \"Cap'ain\", '.', '-PRON-', 'get', 'the', 'scurvy', '.', 'stage', '12', '!']\n",
            "blob:  ['Me', 'sorry', \"Cap'ain\", 'You', 'got', 'the', 'scurvy', 'Stage', '12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RKd2dMlUbOZ",
        "outputId": "0ee05f0e-4e39-4ec9-88ad-a982d151f800"
      },
      "source": [
        "#특징 찾기3 Stemming\n",
        "#어간: 어형 변화의 기초가 되는 부분\n",
        "\n",
        "text=\"Me charge cancellation fee\"\n",
        "spanish_text=\"Yo cobrar cuota de cancelación\"\n",
        "x=word_tokenize(text)\n",
        "spanish_x=word_tokenize(spanish_text)\n",
        "\n",
        "#Porter: 자음+(모음+자음)의 반복+모음 연쇄, 국룰..?\n",
        "#https://tartarus.org/martin/PorterStemmer/python.txt\n",
        "print(\"Porter: \",[PorterStemmer().stem(w) for w in x])\n",
        "\n",
        "#Lancaster: 더 '공격적', 처리속도빠르고 긴 단어에서 오류 많이나는 듯함\n",
        "l=LancasterStemmer()\n",
        "print(\"Lancaster: \", [LancasterStemmer().stem(w) for w in x])\n",
        "\n",
        "#Snowball: 언어 지원\n",
        "print(\"Snowball: \",[nltk.stem.SnowballStemmer('english').stem(w) for w in x])\n",
        "print(\"Snowball_esp: \",[nltk.stem.SnowballStemmer('spanish').stem(w) for w in spanish_x]) #cobrar: 수집하다, yo cobro\n",
        "\n",
        "#Regexp: 사용자 지정 정규식\n",
        "from nltk.stem.regexp import RegexpStemmer as reg\n",
        "print(\"Regexp: \",reg(\"e\").stem(text).split())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter:  ['Me', 'charg', 'cancel', 'fee']\n",
            "Lancaster:  ['me', 'charg', 'cancel', 'fee']\n",
            "Snowball:  ['me', 'charg', 'cancel', 'fee']\n",
            "Snowball_esp:  ['yo', 'cobr', 'cuot', 'de', 'cancel']\n",
            "Regexp:  ['M', 'charg', 'cancllation', 'f']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKM8jYeiMHT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa711a7b-a453-418c-c3c8-89590afe35bd"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "#불용어: 의미분석에 기여하지 않는 단어(쓸데없는 단어)\n",
        "\n",
        "text=\"I will bring them an opera of death.\"\n",
        "print([w for w in word_tokenize(text) if w not in stopwords.words('english')])\n",
        "print([w for w in word_tokenize(text) if w not in stopwords.words('english')+['death']])\n",
        "type(stopwords.words('english'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'bring', 'opera', 'death', '.']\n",
            "['I', 'bring', 'opera', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lel4vV_GlQSY",
        "outputId": "02f84d31-2694-450c-c5cc-911194dbe793"
      },
      "source": [
        "#Rose of Sharyn 가사\n",
        "text='''Numb and broken, here I stand alone.\n",
        "Wondering what were the last words I said to you.\n",
        "It won't be long. We'll meet again.\n",
        "\n",
        "What would I give to behold the smile, the face of love.\n",
        "You never left me. The rising sun will always speak your name.\n",
        "\n",
        "It won't be long, we'll meet again.\n",
        "Your memory is never passing.\n",
        "It won't be long, we'll meet again.\n",
        "My love for you is everlasting.'''\n",
        "a=sent_tokenize(text)\n",
        "for x in a:\n",
        "  print(x,'end')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numb and broken, here I stand alone. end\n",
            "Wondering what were the last words I said to you. end\n",
            "It won't be long. end\n",
            "We'll meet again. end\n",
            "What would I give to behold the smile, the face of love. end\n",
            "You never left me. end\n",
            "The rising sun will always speak your name. end\n",
            "It won't be long, we'll meet again. end\n",
            "Your memory is never passing. end\n",
            "It won't be long, we'll meet again. end\n",
            "My love for you is everlasting. end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t-XNuL2lQUn",
        "outputId": "519873cc-0bd7-469c-a9ed-3905157b3d98"
      },
      "source": [
        "def integer_encoding(text):\n",
        "  vocab = {} # 파이썬의 dictionary 자료형\n",
        "  sentences = []\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for i in text:\n",
        "      sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
        "      result = []\n",
        "      for word in sentence: \n",
        "          word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
        "          if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
        "              if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
        "                  result.append(word)\n",
        "                  if word not in vocab:\n",
        "                      vocab[word] = 0 \n",
        "                  vocab[word] += 1\n",
        "      sentences.append(result)\n",
        "  return (sentences, vocab)\n",
        "sentences=integer_encoding(a)[0]\n",
        "vocab=sorted(integer_encoding(a)[1].items(),key=lambda x: x[1],reverse=True)\n",
        "print(vocab)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"n't\", 3), ('long', 3), (\"'ll\", 3), ('meet', 3), ('love', 2), ('never', 2), ('numb', 1), ('broken', 1), ('stand', 1), ('alone', 1), ('wondering', 1), ('last', 1), ('words', 1), ('said', 1), ('would', 1), ('give', 1), ('behold', 1), ('smile', 1), ('face', 1), ('left', 1), ('rising', 1), ('sun', 1), ('always', 1), ('speak', 1), ('name', 1), ('memory', 1), ('passing', 1), ('everlasting', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6_U6PeeH5sW",
        "outputId": "0600960b-2d4f-4226-d826-dec74b056d98"
      },
      "source": [
        "def wordtoindex(vocab):\n",
        "  word_to_index={}\n",
        "  i=1\n",
        "  for x in vocab:\n",
        "    if x[1]>1:\n",
        "      if x[0] not in word_to_index.keys():\n",
        "        word_to_index[x[0]]=i\n",
        "        i+=1\n",
        "  return word_to_index\n",
        "words_to_index=wordtoindex(vocab)\n",
        "print(words_to_index)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"n't\": 1, 'long': 2, \"'ll\": 3, 'meet': 4, 'love': 5, 'never': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWjrzNeVlQW2",
        "outputId": "60dd8432-43c8-4639-8b78-68614c340d1e"
      },
      "source": [
        "from collections import Counter\n",
        "words=[x for y in sentences for x in y]\n",
        "print(Counter(words))\n",
        "print(Counter(words)[\"alone\"])\n",
        "print(Counter(words).most_common(5))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({\"n't\": 3, 'long': 3, \"'ll\": 3, 'meet': 3, 'love': 2, 'never': 2, 'numb': 1, 'broken': 1, 'stand': 1, 'alone': 1, 'wondering': 1, 'last': 1, 'words': 1, 'said': 1, 'would': 1, 'give': 1, 'behold': 1, 'smile': 1, 'face': 1, 'left': 1, 'rising': 1, 'sun': 1, 'always': 1, 'speak': 1, 'name': 1, 'memory': 1, 'passing': 1, 'everlasting': 1})\n",
            "1\n",
            "[(\"n't\", 3), ('long', 3), (\"'ll\", 3), ('meet', 3), ('love', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13N5nZVBlQZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f168d2b-e660-428d-d441-bce4d22a4174"
      },
      "source": [
        "wordtoindex(Counter(words).most_common(5))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'ll\": 3, 'long': 2, 'love': 5, 'meet': 4, \"n't\": 1}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t5m8JkUlQbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ef1171-75fd-468c-a853-9686b09cebf2"
      },
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "wordsfreq=FreqDist(words)\n",
        "wordsfreq.most_common(5)\n",
        "word_to_index={word[0]: index+1 for index, word in enumerate(wordsfreq.most_common(5))}\n",
        "word_to_index"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'ll\": 3, 'long': 2, 'love': 5, 'meet': 4, \"n't\": 1}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_PVXIKpGvPK",
        "outputId": "4115a887-8828-4e7a-c0a0-fb5cce12b448"
      },
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "print(word_to_index)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"n't\": 1, 'long': 2, \"'ll\": 3, 'meet': 4, 'love': 5, 'OOV': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0ROJxyOG2TR",
        "outputId": "c5b7ad09-f875-4546-cd3a-9bd63ba4f258"
      },
      "source": [
        "encoded = []\n",
        "for s in sentences:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word_to_index[w])\n",
        "        except KeyError:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "    encoded.append(temp)\n",
        "print(encoded)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6, 6, 6, 6], [6, 6, 6, 6], [1, 2], [3, 4], [6, 6, 6, 6, 6, 5], [6, 6], [6, 6, 6, 6, 6], [1, 2, 3, 4], [6, 6, 6], [1, 2, 3, 4], [5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhUTIa84KCG9",
        "outputId": "5710766a-e311-4bcf-ecf8-0c389fe2c451"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=6,oov_token='OOV') #상위 5개, OOV는 자동으로 1\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "tfsent=tokenizer.texts_to_sequences(sentences)\n",
        "print([x for x in tokenizer.word_index.items() if x[1]<6])\n",
        "print(tokenizer.word_counts)\n",
        "print(tfsent)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('OOV', 1), (\"n't\", 2), ('long', 3), (\"'ll\", 4), ('meet', 5)]\n",
            "OrderedDict([('numb', 1), ('broken', 1), ('stand', 1), ('alone', 1), ('wondering', 1), ('last', 1), ('words', 1), ('said', 1), (\"n't\", 3), ('long', 3), (\"'ll\", 3), ('meet', 3), ('would', 1), ('give', 1), ('behold', 1), ('smile', 1), ('face', 1), ('love', 2), ('never', 2), ('left', 1), ('rising', 1), ('sun', 1), ('always', 1), ('speak', 1), ('name', 1), ('memory', 1), ('passing', 1), ('everlasting', 1)])\n",
            "[[1, 1, 1, 1], [1, 1, 1, 1], [2, 3], [4, 5], [1, 1, 1, 1, 1, 1], [1, 1], [1, 1, 1, 1, 1], [2, 3, 4, 5], [1, 1, 1], [2, 3, 4, 5], [1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLtxNidKLh8H",
        "outputId": "46315bd7-de51-4c97-dfc7-4a3a3b6c7ab0"
      },
      "source": [
        "def padding(sent):\n",
        "  maxlength=max(len(item) for item in sent)\n",
        "  for item in sent:\n",
        "    while len(item)<maxlength:\n",
        "      item.append(0)\n",
        "  return sent\n",
        "np.array(padding(tfsent))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 0, 0],\n",
              "       [1, 1, 1, 1, 0, 0],\n",
              "       [2, 3, 0, 0, 0, 0],\n",
              "       [4, 5, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1],\n",
              "       [1, 1, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 0],\n",
              "       [2, 3, 4, 5, 0, 0],\n",
              "       [1, 1, 1, 0, 0, 0],\n",
              "       [2, 3, 4, 5, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM_QGmRsMVUg",
        "outputId": "689c9261-b91d-4838-dcc2-a263710d4994"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tfsent=tokenizer.texts_to_sequences(sentences)\n",
        "pad_sequences(tfsent,padding='post',maxlen=5,truncating='post',value=111)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,   1,   1,   1, 111],\n",
              "       [  1,   1,   1,   1, 111],\n",
              "       [  2,   3, 111, 111, 111],\n",
              "       [  4,   5, 111, 111, 111],\n",
              "       [  1,   1,   1,   1,   1],\n",
              "       [  1,   1, 111, 111, 111],\n",
              "       [  1,   1,   1,   1,   1],\n",
              "       [  2,   3,   4,   5, 111],\n",
              "       [  1,   1,   1, 111, 111],\n",
              "       [  2,   3,   4,   5, 111],\n",
              "       [  1,   1, 111, 111, 111]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VfMtTa5TMUz",
        "outputId": "c586f921-1cf5-4cef-9863-90d49a06ccc8"
      },
      "source": [
        "word2index={}\n",
        "i=0\n",
        "for x in nltk.word_tokenize(a[0]):\n",
        "  if x not in word2index.keys():\n",
        "    word2index[x]=i\n",
        "    i+=1\n",
        "print(word2index)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Numb': 0, 'and': 1, 'broken': 2, ',': 3, 'here': 4, 'I': 5, 'stand': 6, 'alone': 7, '.': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7poLEDoUTlB"
      },
      "source": [
        "def one_hot_encoding(word, word2index):\n",
        "       one_hot_vector = [0]*(len(word2index))\n",
        "       index=word2index[word]\n",
        "       one_hot_vector[index]=1\n",
        "       return one_hot_vector"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU0f5P7qUU8b",
        "outputId": "4d53d9a0-86fd-4808-bec2-159a62725565"
      },
      "source": [
        "one_hot_encoding(\"and\",word2index)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qiK-ytOUat8",
        "outputId": "9f52827f-9e70-4e88-8a5a-69984c0541f2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "t=Tokenizer()\n",
        "t.fit_on_texts([a[0]])\n",
        "t.word_index"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alone': 7, 'and': 2, 'broken': 3, 'here': 4, 'i': 5, 'numb': 1, 'stand': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBHqpAwoVGwd",
        "outputId": "38e81f5c-be80-4795-f915-a3fae2ceaf90"
      },
      "source": [
        "subtext='here stand alone'\n",
        "to_categorical(t.texts_to_sequences([subtext])[0])\n",
        "#일반적으론 단어간 유사도 부여 불가 고양이<->호랑이 vs 고양이<->고정"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcH_0g8VETvf",
        "outputId": "f7341aaf-43c3-4b36-d372-8472d451d2ef"
      },
      "source": [
        "#https://babynames.com/top-baby-names-of-2020\n",
        "\n",
        "text='''1UP OliverUP Amelia/Emilia\n",
        "2DOWN LiamDOWN Charlotte\n",
        "3SAME TheodoreUP Aurora\n",
        "4UP EthanDOWN Violet\n",
        "5BIGUP Aidan/AidenUP Olivia/Alivia\n",
        "6UP BenjaminSAME Ava\n",
        "7DOWN DeclanDOWN Aria/Arya\n",
        "8BIGUP GabrielUP Luna\n",
        "9DOWN Finn/FynnUP Isla\n",
        "10UP ElijahDOWN Hazel\n",
        "11DOWN HenrySAME Scarlett\n",
        "12DOWN OwenUP Maeve\n",
        "13DOWN CalebDOWN Nora/Norah\n",
        "14DOWN AlexanderBIGUP Sophia/Sofia\n",
        "15DOWN Grayson/GreysonBIGUP Chloe\n",
        "16UP SebastianUP Freya/Freja\n",
        "17DOWN LeoUP Abigail\n",
        "18DOWN LeviBIGUP Audrey\n",
        "19UP JasperBIGUP Grace\n",
        "20UP Lucas/LukasDOWN Adelaide\n",
        "21SAME Jackson/JaxonSAME Lily/Lilly\n",
        "22BIGDOWN EmmettBIGUP Isabella\n",
        "23BIGUP Miles/MylesBIGDOWN Vivian\n",
        "24DOWN EverettDOWN Eleanor\n",
        "25UP FelixBIGUP Ella\n",
        "26UP LandonBIGUP Ophelia\n",
        "27BIGUP AtticusUP Adeline\n",
        "28BIGUP MiloBIGDOWN Genevieve\n",
        "29UP JackDOWN Ivy\n",
        "30BIGUP TheoBIGDOWN Lucy\n",
        "31BIGDOWN Elliot/ElliottDOWN Emma\n",
        "32DOWN ArcherBIGUP Hannah/Hanna\n",
        "33DOWN SilasBIGUP Iris\n",
        "34BIGUP IsaacBIGUP Esme\n",
        "35UP ArthurBIGDOWN Penelope\n",
        "36BIGUP EliasDOWN Clara\n",
        "37BIGDOWN WilliamUP Cora\n",
        "38DOWN WyattDOWN Eloise\n",
        "39NEW CaydenSAME Mila\n",
        "40DOWN NoahUP Astrid\n",
        "41BIGUP MatteoBIGUP Arabella\n",
        "42BIGUP SamuelBIGDOWN Naomi\n",
        "43DOWN HudsonDOWN Elizabeth\n",
        "44UP HarrisonNEW Isabelle\n",
        "45UP AugustBIGUP Evangeline\n",
        "46NEW Alastair/AlistairBIGDOWN Ruby\n",
        "47BIGDOWN GavinUP Sadie\n",
        "48BIGUP JacobUP Mia\n",
        "49DOWN LincolnBIGUP Maya/Maia\n",
        "50BIGUP AaronNEW Keira\n",
        "51BIGUP EzraUP Wren\n",
        "52DOWN AtlasBIGUP Autumn\n",
        "53UP ColeBIGUP Gemma/Jemma\n",
        "54BIGUP Xander/ZanderBIGUP Ada\n",
        "55BIGUP TobiasNEW Delilah\n",
        "56BIGDOWN HoldenBIGDOWN Rosalie\n",
        "57DOWN NathanielBIGDOWN Lorelei\n",
        "58BIGUP DominicDOWN Eliza\n",
        "59BIGUP IsaiahBIGDOWN Anastasia\n",
        "60BIGUP Adrian/AdrienDOWN Josephine\n",
        "61BIGDOWN RhysBIGDOWN Claire/Clare\n",
        "62UP GideonDOWN Rose\n",
        "63UP DanielBIGDOWN Lydia\n",
        "64UP LukeBIGUP Kaia/Kaya\n",
        "65BIGDOWN JamesBIGUP Saoirse\n",
        "66NEW Connor/ConorUP Layla\n",
        "67BIGUP XavierBIGDOWN Lila/Lilah\n",
        "68BIGDOWN CallumDOWN Margot/Margo\n",
        "69UP ArloBIGUP Phoebe\n",
        "70BIGDOWN LoganBIGDOWN Stella\n",
        "71DOWN AxelBIGUP Zoe\n",
        "72NEW EdwardBIGDOWN Juliet/Juliette\n",
        "73BIGUP ThomasBIGUP Aurelia\n",
        "74NEW EricBIGUP Daisy\n",
        "75NEW MatthewUP Athena\n",
        "76SAME ZacharyDOWN Thea\n",
        "77BIGDOWN AsherBIGDOWN Juniper\n",
        "78BIGUP EliBIGDOWN Alice\n",
        "79UP BeauDOWN Madeline\n",
        "80BIGDOWN RomanBIGDOWN Natalie\n",
        "81BIGDOWN NolanBIGDOWN Fiona\n",
        "82DOWN NicholasUP Sienna\n",
        "83UP LachlanNEW Eliana\n",
        "84BIGDOWN NathanNEW Willow\n",
        "85BIGDOWN RonanNEW Vera\n",
        "86UP AdamBIGDOWN Matilda\n",
        "87BIGDOWN JonahDOWN Emily\n",
        "88NEW AceUP Olive\n",
        "89BIGDOWN AndrewUP Dahlia\n",
        "90UP ColtonNEW Anna\n",
        "91NEW AbelNEW Mabel\n",
        "92BIGDOWN CharlesNEW Eva\n",
        "93NEW MichaelBIGDOWN Evelyn\n",
        "94NEW MalachiNEW Leilani\n",
        "95NEW VincentBIGDOWN Harper\n",
        "96NEW CaspianBIGDOWN Daphne\n",
        "97NEW OscarNEW Sophie\n",
        "98UP IanNEW Amara\n",
        "99BIGDOWN RowanDOWN Brielle\n",
        "100NEW Luca/LukaDOWN Aaliyah'''\n",
        "import re\n",
        "j=re.findall(\"[A-Z][a-z]+/?\",text)\n",
        "a=1\n",
        "t=[]\n",
        "tlen=[]\n",
        "for x in j:\n",
        "  if '/' not in x:    #불필요한 단어 제거 (원래는 등장빈도가 낮거나 길이가 짧은 경우/불용어일 경우 제거할 때가 많음)\n",
        "    if a==0:\n",
        "       a=1\n",
        "    elif a==1:\n",
        "       a=0\n",
        "    t.append([x,a])\n",
        "    if len(x)>=6: #불필요한 단어 제거 (길이가 짧을 때)\n",
        "       tlen.append([x.lower(),a])\n",
        "print(t[:5])\n",
        "print(tlen[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Oliver', 0], ['Emilia', 1], ['Liam', 0], ['Charlotte', 1], ['Theodore', 0]]\n",
            "[['oliver', 0], ['emilia', 1], ['charlotte', 1], ['theodore', 0], ['aurora', 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig4PaqzmIR7o",
        "outputId": "05cd7454-079f-4b76-8b70-7b6ba8728f22"
      },
      "source": [
        "X,y=zip(*t)\n",
        "print(X[:5])\n",
        "print(y[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('oliver', 'emilia', 'liam', 'charlotte', 'theodore')\n",
            "(0, 1, 0, 1, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "4cb8L1SsITGt",
        "outputId": "dfcfd7f0-fc7e-470c-bd15-334531c49c1c"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(t,columns=['name','01'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oliver</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>emilia</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>liam</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>charlotte</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>theodore</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        name  01\n",
              "0     oliver   0\n",
              "1     emilia   1\n",
              "2       liam   0\n",
              "3  charlotte   1\n",
              "4   theodore   0"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WtC5mNGIlYS",
        "outputId": "8a90ff79-6596-461c-95a2-564dbab8cbe0"
      },
      "source": [
        "X=df.iloc[:,0]\n",
        "y=df.iloc[:,1]\n",
        "print(tuple(X)[:5])\n",
        "print(tuple(y)[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('oliver', 'emilia', 'liam', 'charlotte', 'theodore')\n",
            "(0, 1, 0, 1, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuwRBhNiJEB5",
        "outputId": "1c305570-7971-4826-fc56-95564f3947a9"
      },
      "source": [
        "n_of_train = int(len(X) * 0.8)\n",
        "X_test = X[n_of_train:] \n",
        "y_test = y[n_of_train:] \n",
        "X_train = X[:n_of_train] \n",
        "y_train = y[:n_of_train]\n",
        "print(len(X_test))\n",
        "print(len(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xP3eBBo6JaUx",
        "outputId": "83bac90d-7ad5-4459-f43b-9770eb796a2f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)\n",
        "print(len(X_test))\n",
        "print(len(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMYuY0tf8DBz"
      },
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Kkma, Okt\n",
        "kkma=Kkma()\n",
        "okt=Okt()"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPTtllljKmZL",
        "outputId": "16ebc4ce-e142-4f86-8c69-a649eb0ab38b"
      },
      "source": [
        "print(okt.morphs(\"혈액 샘플 필요하다!\"))\n",
        "print(okt.pos(\"혈액 샘플 필요하다!\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['혈액', '샘플', '필요하다', '!']\n",
            "[('혈액', 'Noun'), ('샘플', 'Noun'), ('필요하다', 'Adjective'), ('!', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OOas9h08Fr-",
        "outputId": "0ebabe29-fcaa-430f-e7b7-b0313b59bed3"
      },
      "source": [
        "print(okt.morphs(\"제세동기르르릃르르릃르르릃.\"))\n",
        "print(okt.pos(\"제세동기르르릃르르릃르르릃.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['제세동기', '르르', '릃르르릃르르릃', '.']\n",
            "[('제세동기', 'Noun'), ('르르', 'Noun'), ('릃르르릃르르릃', 'Noun'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVSusjkqiKVI"
      },
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAZCq6OhkMLX"
      },
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NptpgyRl8HM"
      },
      "source": [
        "!pip install soynlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wqwDScaquPo"
      },
      "source": [
        "!pip install customized_konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO--pyHWjOPB",
        "outputId": "597f6784-29f0-4364-c64b-17e33baa98c8"
      },
      "source": [
        "text=\"혈액 샘플 필요하다!\".replace(\" \",'')\n",
        "texttwo=\"외않되?\"\n",
        "from pykospacing import Spacing\n",
        "from hanspell import spell_checker\n",
        "spacing=Spacing()\n",
        "print(text)\n",
        "print(spacing(text))\n",
        "print(spell_checker.check(texttwo).checked)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "혈액샘플필요하다!\n",
            "혈액 샘플 필요하다!\n",
            "왜 안돼?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoWjFi7aT-an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa93f391-c052-4d2e-cf76-6b7150132b10"
      },
      "source": [
        "#Soynlp 응집확률(문자를 문자열 단위로 분리 후 순서대로 추가하면서 가장 높은 확률이 단어로 나올 가능성이 높다!)\n",
        "#비지도 학습 기반(데이터 필요)\n",
        "#아래와 같이 바뀌어가는 언어문화에 적응하기 위한 접근\n",
        "#카카오톡은 11년부터 자주 쓰이기 시작한 단어\n",
        "print(Okt().morphs(\"무야호... 야호... 그만큼 신나시는 거지~\")) \n",
        "print(Okt().morphs(\"카카오톡으로 만나는 단골고객 카카오톡 스토어, 지금 새로운 단골고객을 만들어 보세요.\")) "
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['무', '야호', '...', '야호', '...', '그만큼', '신나시는', '거지', '~']\n",
            "['카카오', '톡', '으로', '만나는', '단골', '고객', '카카오', '톡', '스토어', ',', '지금', '새로운', '단골', '고객', '을', '만들어', '보세요', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGKQOezmkOgn",
        "outputId": "73bb7e1b-2597-4acb-bca8-c2c4bc1825d1"
      },
      "source": [
        "import urllib.request\n",
        "from soynlp import DoublespaceLineCorpus\n",
        "from soynlp.word import WordExtractor\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")\n",
        "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
        "len(corpus)\n",
        "word_extractor = WordExtractor()\n",
        "word_extractor.train(corpus)\n",
        "word_score_table = word_extractor.extract()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training was done. used memory 1.906 Gb\n",
            "all cohesion probabilities was computed. # words = 223348\n",
            "all branching entropies was computed # words = 361598\n",
            "all accessor variety was computed # words = 361598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3iiU_VBoD9e",
        "outputId": "6ab91dfc-b5ec-4b5d-a68d-8e05f297ea90"
      },
      "source": [
        "print(word_score_table[\"카카\"].cohesion_forward)\n",
        "print(word_score_table[\"카카\"].right_branching_entropy)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1386575208231259\n",
            "-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_2FMTgfoZL7",
        "outputId": "f1a6acfd-4915-490b-8164-e5727364729c"
      },
      "source": [
        "print(word_score_table[\"카카오\"].cohesion_forward)\n",
        "print(word_score_table[\"카카오\"].right_branching_entropy)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3723674540331444\n",
            "2.1028544930905926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPY8jSc6muag",
        "outputId": "fcf987dd-8ca0-417a-b91c-9fcca9995c4e"
      },
      "source": [
        "print(word_score_table[\"카카오톡\"].cohesion_forward)\n",
        "print(word_score_table[\"카카오톡\"].right_branching_entropy)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.40341675456207704\n",
            "2.2478442027083916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlmoTij1ovaM",
        "outputId": "19cd0319-55a0-4879-899a-8f8521ccda86"
      },
      "source": [
        "from soynlp.tokenizer import LTokenizer\n",
        "from soynlp.tokenizer import MaxScoreTokenizer\n",
        "#LTokenizer는 어절 분리(L,R)\n",
        "#MaxScore는 띄어쓰기가 안되어있을 때\n",
        "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
        "l_tokenizer = LTokenizer(scores=scores)\n",
        "print(l_tokenizer.tokenize(\"카카오톡으로 만나는 단골고객 카카오톡 스토어, 지금 새로운 단골고객을 만들어 보세요.\", flatten=False))\n",
        "maxscore_tokenizer = MaxScoreTokenizer(score  s=scores)\n",
        "print(maxscore_tokenizer.tokenize(\"카카오톡으로만나는단골고객카카오톡스토어,지금새로운단골고객을만들어보세요.\"))"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('카카오톡', '으로'), ('만나는', ''), ('단골고객', ''), ('카카오톡', ''), ('스토어', ','), ('지금', ''), ('새로운', ''), ('단골고객을', ''), ('만들어', ''), ('보세요.', '')]\n",
            "['카카오톡', '으로', '만나는', '단골', '고객', '카카오톡', '스토어', ',', '지금', '새로운', '단골', '고객', '을만', '들어', '보세요.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bUWXh0golbr",
        "outputId": "3df3c123-4d8b-4694-83e4-d6c89841985e"
      },
      "source": [
        "from soynlp.normalizer import *\n",
        "text1=\"ㅋㅋㅋㅋㅋ카톡보셈\"\n",
        "text2=\"ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ카톡보셈\"\n",
        "text3=\"크크크크크이따톡보셈\"\n",
        "example=[text1,text2,text3]\n",
        "for x in example:\n",
        "  print(emoticon_normalize(x,num_repeats=3))\n",
        "  print(repeat_normalize(x,num_repeats=2))\n",
        "  print()"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ㅋㅋㅋㅏ톡보셈\n",
            "ㅋㅋ카톡보셈\n",
            "\n",
            "ㅋㅋㅋㅏ톡보셈\n",
            "ㅋㅋ카톡보셈\n",
            "\n",
            "크크크이따톡보셈\n",
            "크크이따톡보셈\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g67hgxLUqqxQ",
        "outputId": "8318d103-24ef-44f5-8a73-5a14c3ee16a2"
      },
      "source": [
        "from ckonlpy.tag import Twitter #여기엔 Okt가 없는 듯..합니다\n",
        "twitter=Twitter()\n",
        "print(twitter.morphs('무야호... 야호... 그만큼 신나시는 거지~'))\n",
        "twitter.add_dictionary('무야호','Noun')\n",
        "print(twitter.morphs('무야호... 야호... 그만큼 신나시는 거지~'))"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['무', '야호', '...', '야호', '...', '그', '만큼', '신', '나시', '는', '거지', '~']\n",
            "['무야호', '...', '야호', '...', '그', '만큼', '신', '나시', '는', '거지', '~']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaDqzNEOwL7_"
      },
      "source": [
        "#PDF 다운로드 링크: https://drive.google.com/file/d/1ozsOG9U8jNWePCVf9cbV5p7_hNtcF6eH/view?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210903 (Text Preprocessing)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEq2NXGrlwZ"
      },
      "source": [
        "##텍스트 전처리(Text Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZosxLpLkrtwY"
      },
      "source": [
        "컴퓨터가 자연어를 이해하도록 학습시키기 위해서 시소러스부터 word2vec까지 다양한 형태의 기법이 사용된다. 위 과정에서 효율적인 연산을 돕기 위해 corpus(말뭉치: 대량의 텍스트 데이터)를 전처리 하는 과정이 필요하다. 각 단계는 corpus를 자르고, 정제하고, 통합함으로써 의미를 파악하는데 핵심적인 요소만을 남기는데 목적을 두고 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXW5pvQxvjrA"
      },
      "source": [
        "NPL 학습의 가장 큰 착안점은 단어의 의미가 맥락에서(주변 단어의 관계에서) 형성된다는 \"분포 가설\"이다. 따라서 문장을 분리하고, 각 단어의 위치를 인덱싱하는 과정이 필요하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StLpY1dXsyIA"
      },
      "source": [
        "###토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9_VSrzos3GP"
      },
      "source": [
        "토큰화는 주어진 텍스트 데이터를 토큰이라 불리는 작은 단위로 쪼개는 작업을 이른다. 토큰의 크기는 상황에 따라 다르지만, 보통 의미를 가지는 언어의 단위(단어, 어구, 문장 등)를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHGZD0MjtjtA"
      },
      "source": [
        "####단어 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1EZi38DtoBm"
      },
      "source": [
        "의미를 가지는 최소 단위인 단어를 기준으로 나누는 것이다. 언어의 특성마다 다르지만, 영어의 경우 뛰어쓰기에 사용되는 \" \"(공백)을 이용해서 나눌 수 있기 때문에 간단히 구현이 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEvf7MpXt1dY",
        "outputId": "d0249be3-3af0-4a83-ab33-4ccdc9930b08"
      },
      "source": [
        "text=\"I like to learn Python.\"\n",
        "text= text.replace('.', ' .')\n",
        "t_list=text.split(\" \")\n",
        "t_list"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'like', 'to', 'learn', 'Python', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4avDbeb-45i",
        "outputId": "e939cec3-711f-41e9-85a2-8dd991eb7af3"
      },
      "source": [
        "text=\"I like to learn Python.\"\n",
        "text=text.lower()\n",
        "import re\n",
        "re.split('(\\W+)',text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', ' ', 'like', ' ', 'to', ' ', 'learn', ' ', 'python', '.', '']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKwP0aiSuYJQ"
      },
      "source": [
        "한국어의 경우 조사를 붙여쓰는 교착어로 어절(띄어쓰기의 단위)이 언제나 단어와 일치하지 않기 때문에 이에 유의하여 토큰화를 진행해야 한다. \n",
        "\n",
        "어절이 아닌 형태소 단위로 나눔으로써 이를 해결할 수 있다. KoNLPy라는 파이썬 패키지를 이용해 간단히 형태소 단위의 토큰화가 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnvrrYqdBq7S",
        "outputId": "82259d33-d489-4f97-9b7d-0096eb95ea3b"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 123 kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 44.4 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f83o9XcwPVQ",
        "outputId": "c2c5323d-ebc0-4118-b13e-020db92cd79c"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "print(okt.morphs(\"한국어로 하는 자연어 처리는 즐거워!\"))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['한국어', '로', '하는', '자연어', '처리', '는', '즐거워', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVUTwuanwWzY"
      },
      "source": [
        "####문장 토큰화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2iT0WTbwkQY"
      },
      "source": [
        "문장 토큰화는 corpus가 하나의 글, 즉 여러 문장인 경우 이를 나누기 위한 작업이다. 문장은 맥락을 가지는 최소의 단위로 한 문장에서의 단어의 절대적, 상대적 위치를 파악하는 것은 그 단어의 성질을 이해하는데 큰 도움이 된다. 가령 주어로 쓰일 수 있는 'I', 'You', 'She'등은 한 문장의 맨 앞에 놓일 가능성이 크다. 기본적으로 문장의 종결을 알리는 기호들('.', '?', '!')을 이용한 분류가 가능하다. 하지만, 위 기호들이 언제나 문장의 끝을 알리는 것은 아니므로 주의해야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUpnAMpQxxrf"
      },
      "source": [
        "마침표는 약어의 표시와 문장종결의 표시 두 역할을 수행한다. 따라서 두 경우를 구별하기 위한 이진 분류기가 필요하다. 마침표 주변에 숫자, @, 또 다른 마침표의 유무 등을 확임함으로써 이 둘을 구별할 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuc-ABRyzZvP"
      },
      "source": [
        "##정제(Cleaning) & 정규화(Normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7PyPK6qzq0A"
      },
      "source": [
        "####정제(Cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duA72EBzztsI"
      },
      "source": [
        "정제는 주어진 corpus에서 문장의 의미 파악에 방해가 되는 혹은 필요가 없는 노이즈 데이터를 제거하는 작업이다. 언어의 특성에 따라 다르겠지만 그 기준으로 낮은 등장 빈도, 짤은 길이 등이 있다. 예를 들어 영어의 관사('a', 'an')등은 특별한 의미를 가지지 않으므로 연산의 효율성을 위해서 제거하는 것이 좋다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfmHELub6Slx"
      },
      "source": [
        "####정규화(Normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUBACoxx6Wq6"
      },
      "source": [
        "정규화는 표현 방법이 다른 단어들을 통합시키는 과정이다. 줄임말이나 영어의 대소문자를 통일 시키는 것이 대표적이다. 예로, 서강대=>서강대학교 로 변환하는 과정 등이 있다. 이를 위해서는 단어의 핵심의미를 가지는 부분들을 추출하는 작업들이 필요하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cvmhy96wuR"
      },
      "source": [
        "####어간 추출(Stemming) & 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqLOIA0i64bp"
      },
      "source": [
        "어간 추출과 표제어 추출은 corpus에 존재하는 단어의 가짓수를 줄이기 위한 작업이다. 다른 형태이지만, 동일한 의미를 가리키게 하는 핵심적인 부분들을 추출함으로써 단어를 통합시킬 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byHauQqQ7cBx"
      },
      "source": [
        "표제어 추출은 다양한 형태로 활용된 단어들을 기본형의 단어로 매칭시키는 작업니다. 가령 명사의 복수형 어미 '-es', '-s'를 제거하거나 동사의 과거형을 기본형으로 바꾸어주는 과정이 있다. 이때 각 단어의 품사를 아는 것이 큰 도움이 된다는 점에서, 품사 태깅이 중요하다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uergTj808AHq"
      },
      "source": [
        "어간 추출은 하나의 단어를 의미구로 자르는 작업을 한다. 이는 한국어의 형태소 분석과 비슷한 역할을 하는데, 새로운 단어의 의미를 기존에 알고 있는 단어로 분리해 연관지을 수 있도록 한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIDg96dK9Ash"
      },
      "source": [
        "###불용어(Stopword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Jn88Ut9Fyp"
      },
      "source": [
        "불용어는 의미 파악에 크게 도움되지 않는 단어 토큰을 제거하는 작업이다. 자주 등장하지만 학습에 크게 도움되지 않는 단어들을 지정해 삭제할 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXaOTK0I9gip"
      },
      "source": [
        "###원-핫 인코딩(One-Hot Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikbKoegZ9mBi"
      },
      "source": [
        "원-핫 인코딩은 컴퓨터가 텍스트들의 관계를 연산하는데 용이하기 위해 토큰에 ID를 부여하는 전처리를 말한다. 단어 토큰화의 경우 문장에서의 위치로 라벨링하여 나타낸다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WAiAYi7-STR",
        "outputId": "783b80a8-0331-4e14-bfdf-c813778667e3"
      },
      "source": [
        "text=\"I like to learn Python.\"\n",
        "text=text.lower()\n",
        "text= text.replace('.', ' .')\n",
        "t_list=text.split(\" \")\n",
        "\n",
        "word_to_id={}\n",
        "id_to_word={}\n",
        "\n",
        "for word in t_list:\n",
        "    if word not in word_to_id:\n",
        "        new_id=len(word_to_id)\n",
        "        word_to_id[word]=new_id\n",
        "        id_to_word[new_id]=word\n",
        "\n",
        "print(word_to_id)\n",
        "print(id_to_word)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 0, 'like': 1, 'to': 2, 'learn': 3, 'python': 4, '.': 5}\n",
            "{0: 'i', 1: 'like', 2: 'to', 3: 'learn', 4: 'python', 5: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV6tJCQoC5Mi"
      },
      "source": [
        "이러한 전처리를 통해서 최빈 단어 순으로 나열하거나, 동시발생행렬을 만들 수 있다. 동시발생 행렬이란 하나의 단어 주변에 다른 단어가 몇번 나타나는지를 표시하는 행렬이다. 주변의 정의(window size), 좌우 방향, 문장의 시작과 끝 등의 요소를 고려해 주변 단어를 집계한다."
      ]
    }
  ]
}
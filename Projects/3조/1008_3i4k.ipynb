{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1008-3i4k.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ59L9HRdi6p",
        "outputId": "a11bf37e-8775-419b-8e45-eada657b36a5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/warnikchow/3i4k/master/data/train_val_test/fci_train_val.txt\n",
        "!wget https://raw.githubusercontent.com/warnikchow/3i4k/master/data/train_val_test/fci_test.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-08 10:01:18--  https://raw.githubusercontent.com/warnikchow/3i4k/master/data/train_val_test/fci_train_val.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2661060 (2.5M) [text/plain]\n",
            "Saving to: ‘fci_train_val.txt.1’\n",
            "\n",
            "\rfci_train_val.txt.1   0%[                    ]       0  --.-KB/s               \rfci_train_val.txt.1 100%[===================>]   2.54M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-10-08 10:01:18 (37.1 MB/s) - ‘fci_train_val.txt.1’ saved [2661060/2661060]\n",
            "\n",
            "--2021-10-08 10:01:18--  https://raw.githubusercontent.com/warnikchow/3i4k/master/data/train_val_test/fci_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 295054 (288K) [text/plain]\n",
            "Saving to: ‘fci_test.txt.1’\n",
            "\n",
            "fci_test.txt.1      100%[===================>] 288.14K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-10-08 10:01:18 (8.99 MB/s) - ‘fci_test.txt.1’ saved [295054/295054]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnAbIHNPdnnU"
      },
      "source": [
        "def read_data(filename):\n",
        "\n",
        "    with open(filename, \"r\", encoding=\"UTF8\") as f:\n",
        "        data = [line.split(\"\\t\") for line in f.read().splitlines()]\n",
        "\n",
        "    return data\n",
        "\n",
        "train_data = read_data(\"/content/fci_train_val.txt\")\n",
        "X_train_data = [t[1] for t in train_data]\n",
        "y_train_data = [int(t[0]) for t in train_data]\n",
        "\n",
        "test_data = read_data(\"/content/fci_test.txt\")\n",
        "X_test_data = [t[1] for t in test_data]\n",
        "y_test_data = [int(t[0]) for t in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg-67cnGZBLj",
        "outputId": "3ca6bcd1-ea75-4bae-c499-d5f62d318d2a"
      },
      "source": [
        "print(len(X_train_data))\n",
        "print(len(y_train_data))\n",
        "print(len(X_test_data))\n",
        "print(len(y_test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55134\n",
            "55134\n",
            "6121\n",
            "6121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRbVDelZdpVv",
        "outputId": "102819e8-00ee-4f0e-f8f6-b735a2d56a3e"
      },
      "source": [
        "!pip install konlpy\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izGue4OBdrHd",
        "outputId": "972b8fd4-2499-4f00-99c8-54c95a868740"
      },
      "source": [
        "import random\n",
        "\n",
        "def swap(new_words):\n",
        "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
        "\trandom_idx_2 = random_idx_1\n",
        "\tcounter = 0\n",
        "\n",
        "\twhile random_idx_2 == random_idx_1:\n",
        "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter > 3:\n",
        "\t\t\treturn new_words\n",
        "\n",
        "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
        "\treturn new_words\n",
        "\n",
        "def random_swap(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\tfor _ in range(n):\n",
        "\t\tnew_words = swap(new_words)\n",
        "\treturn new_words\n",
        "\n",
        "def EDA(sentence, alpha_rs=0.1, num_aug=4):\n",
        "    words = sentence.split(' ')\n",
        "    num_words = len(words)   \n",
        "    augmented_sentences = []\n",
        "    num_new_per_technique = int(num_aug/4) + 1\n",
        "    n_rs = max(1, int(alpha_rs*num_words))\n",
        "\n",
        "    for x in range(num_new_per_technique): \n",
        "        a_words = random_swap(words, n_rs)\n",
        "        augmented_sentences.append(\" \".join(a_words))\n",
        "\n",
        "    if num_aug >= 1:\n",
        "        augmented_sentences = augmented_sentences[:num_aug]\n",
        "    \n",
        "    augmented_sentences.append(sentence)   \n",
        "    return augmented_sentences\n",
        " \n",
        "aug = []\n",
        "for sentence in X_train_data : \n",
        "    augmented_sentences = EDA(sentence, alpha_rs = 0.1, num_aug = 4)\n",
        "    aug.append(augmented_sentences)\n",
        "\n",
        "X_train_aug = []\n",
        "y_train_aug = []\n",
        "\n",
        "for sentence in aug :  \n",
        "    index = aug.index(sentence)\n",
        "    for aug_sentence in sentence : \n",
        "        X_train_aug.append(aug_sentence)\n",
        "        y_train_aug.append(y_train_data[index])\n",
        "    \n",
        "print(len(X_train_aug)) \n",
        "print(len(y_train_aug))\n",
        "\n",
        "## result x_trian_aug 165402 \n",
        "##        y_train_aug 165402"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165402\n",
            "165402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laOLpT0NdzQD"
      },
      "source": [
        "# 형태소 분리\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from konlpy.tag import Okt\n",
        "pos_tagger = Okt()\n",
        "\n",
        "def twit_token(doc):\n",
        "    x = [t[0] for t in pos_tagger.pos(doc)]\n",
        "    return ' '.join(x)\n",
        "\n",
        "len_train = int(np.floor(len(X_train_aug)*0.9))\n",
        "fci_data_train  = X_train_aug[:len_train]\n",
        "fci_data_test   = X_train_aug[len_train:]\n",
        "fci_label_train = y_train_aug[:len_train]\n",
        "fci_label_test  = y_train_aug[len_train:]\n",
        "\n",
        "fci_token_train = [twit_token(row) for row in fci_data_train]\n",
        "fci_token_test  = [twit_token(row) for row in fci_data_test]\n",
        "\n",
        "fci_sp_token_train = [nltk.word_tokenize(row) for row in fci_token_train]\n",
        "fci_sp_token_test  = [nltk.word_tokenize(row) for row in fci_token_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XHt9if0o_vK",
        "outputId": "0d43fe77-d7f1-43e4-90ac-280cdf378750"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.0-py2.py3-none-any.whl (207 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3119389 sha256=3f5399e8cc3b9274f78aea3f15d12f229c04cfae95da491eba3fb3be21cf790a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfqGbAJAWXfx",
        "outputId": "d5e6984f-2b19-4445-b8be-c49a57fb7329"
      },
      "source": [
        "!gdown --id 1jHbjOcnaLourFzNuP47yGQVhBTq6Wgor --output vector.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jHbjOcnaLourFzNuP47yGQVhBTq6Wgor\n",
            "To: /content/vector.zip\n",
            "100% 874M/874M [00:06<00:00, 136MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOfL6Hm0Yb9m",
        "outputId": "9b3cb499-fe61-4b01-faf2-fbd55db75f53"
      },
      "source": [
        "!unzip -qq \"/content/vector.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace model_drama.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlB6PaZ0o-JO"
      },
      "source": [
        "#import fasttext\n",
        "#model_ft = fasttext.load_model('content/model_drama.bin')\n",
        "# In case fasttext is not installed\n",
        "# use FastText wrapper in Gensim as:\n",
        "from gensim.models.wrappers import FastText\n",
        "model_ft = FastText.load_fasttext_format('model_drama.bin')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luHHgQFPqAqG"
      },
      "source": [
        "def featurize_cnn(corpus,wdim,maxlen):\n",
        "    conv_total = np.zeros((len(corpus),maxlen,wdim,1))\n",
        "    for i in range(len(corpus)):\n",
        "        if i%1000 ==0:\n",
        "            print(i)\n",
        "        s = corpus[i]\n",
        "        for j in range(len(s)):\n",
        "            if s[-j-1] in model_ft and j < maxlen:\n",
        "                conv_total[i][-j-1,:,0] = model_ft[s[-j-1]]\n",
        "    return conv_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW4mJIqJYvRb"
      },
      "source": [
        "fci_conv = featurize_cnn(fci_sp_token_train+fci_sp_token_test,100,30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl72dF3maJ2Y"
      },
      "source": [
        "tfidf로 시도했던 내용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHuBAZh5aJL2"
      },
      "source": [
        "# Referred to the followings for the code:\n",
        "# https://gist.github.com/jason-riddle/1a854af26562c0cdb1e6ff550d1bf32d#file-complex-tf-idf-example-py-L40\n",
        "# http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidf_vectorizer 함수는 train corpus에서 tf-idf statistics를 뽑아내고 이를 이용해 train corpus+test corpus를 수치화하는 과정\n",
        "def tfidf_featurizer(corpus_train,corpus_test):\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    count_vectorizer.fit_transform(corpus_train)\n",
        "    freq_term_matrix = count_vectorizer.transform(corpus_train)\n",
        "    tfidf = TfidfTransformer(norm=\"l2\")\n",
        "    tfidf.fit(freq_term_matrix)\n",
        "\n",
        "    # unigram features\n",
        "    tfidf_token = TfidfVectorizer(ngram_range=(1,1),max_features=3000)\n",
        "    token_tfidf_total = tfidf_token.fit_transform(corpus_train+corpus_test)\n",
        "    token_tfidf_total_mat = token_tfidf_total.toarray()\n",
        "\n",
        "    # bigram features\n",
        "    tfidf_bi_token = TfidfVectorizer(ngram_range=(1,2),max_features=3000)\n",
        "    token_tfidf_bi_total = tfidf_bi_token.fit_transform(corpus_train+corpus_test)\n",
        "    token_tfidf_bi_total_mat = token_tfidf_bi_total.toarray()\n",
        "    return token_tfidf_total_mat,token_tfidf_bi_total_mat\n",
        "\n",
        "fci_tfidf,fci_tfidf_bi  = tfidf_featurizer(fci_token_train,fci_token_test)\n",
        "fci_tfidf_train    = fci_tfidf[:len_train]\n",
        "fci_tfidf_test     = fci_tfidf[len_train:]\n",
        "fci_tfidf_bi_train = fci_tfidf_bi[:len_train]\n",
        "fci_tfidf_bi_test  = fci_tfidf_bi[len_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUR5SN45aWhc"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "classifier_uni = LogisticRegression(random_state=1234)\n",
        "classifier_uni.fit(fci_tfidf_train,fci_label_train)\n",
        "uni_pred = classifier_uni.predict(fci_tfidf_test)\n",
        "\n",
        "accuracy_score(uni_pred,fci_label_test)\n",
        "metrics.f1_score(uni_pred,fci_label_test,average=\"macro\")\n",
        "metrics.f1_score(uni_pred,fci_label_test,average=\"weighted\")\n",
        "precision_recall_fscore_support(uni_pred,fci_label_test)\n",
        "\n",
        "classifier_bi = LogisticRegression(random_state=1234)\n",
        "classifier_bi.fit(fci_tfidf_bi_train,fci_label_train)\n",
        "bi_pred = classifier_bi.predict(fci_tfidf_test)\n",
        "\n",
        "accuracy_score(bi_pred,fci_label_test)\n",
        "metrics.f1_score(bi_pred,fci_label_test,average=\"macro\")\n",
        "metrics.f1_score(bi_pred,fci_label_test,average=\"weighted\")\n",
        "precision_recall_fscore_support(bi_pred,fci_label_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoOW6Ffhacnk"
      },
      "source": [
        "1st try result\n",
        "\n",
        "acc: 0.7665941240478781\n",
        "\n",
        "f1_score_macro: 0.621190123136072\n",
        "\n",
        "f1_score_weighted: 0.7752608151283553\n",
        "\n",
        "\n",
        "2nd try result\n",
        "\n",
        "acc: 0.7680309533885497\n",
        "\n",
        "f1_score_macro: 0.6391839862416706\n",
        "\n",
        "f1_score_weighted: 0.7741350836238543"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoQb0gcJadAk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}